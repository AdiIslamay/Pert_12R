{
    "type": [
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        0,
        1,
        3,
        3,
        3,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        3,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        3,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        2
    ],
    "data": [
        "> ",
        "#tahap 1",
        "> ",
        "# Install",
        "> ",
        "install.packages(\"tm\")  # for text mining",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "install.packages(\"SnowballC\") # for text stemming",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "install.packages(\"wordcloud\") # word-cloud generator ",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "install.packages(\"RColorBrewer\") # color palettes",
        "Error in install.packages : Updating loaded packages\n",
        "> ",
        "",
        "> ",
        "# Load",
        "> ",
        "library(\"tm\")",
        "> ",
        "library(\"SnowballC\")",
        "> ",
        "library(\"wordcloud\")",
        "> ",
        "library(\"RColorBrewer\")",
        "> ",
        "library(stringr)",
        "> ",
        "",
        "> ",
        "setwd(\"F:/DOC/lionGithub\")",
        "Error in setwd(\"F:/DOC/lionGithub\") : cannot change working directory\n",
        "> ",
        "docs<-readLines(\"datalion.csv\")",
        "Error in file(con, \"r\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(con, \"r\") :",
        "\n ",
        " cannot open file 'datalion.csv': No such file or directory\n",
        "> ",
        "",
        "> ",
        "# Load the data as a corpus",
        "> ",
        "docs <- Corpus(VectorSource(docs))",
        "> ",
        "",
        "> ",
        "#Inspect the content of the document",
        "> ",
        "inspect(docs)",
        "<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] c(\"\", \" even though face difficulties today tomorrow still dream dream deeply rooted american dream\", \" \", \" dream one day nation will rise live true meaning creed\", \" \", \" hold truths selfevident men created equal\", \" \", \" dream one day red hills georgia sons former slaves sons former slave owners will able sit table brotherhood\", \" \", \" dream one day even state mississippi state sweltering heat injustice sweltering heat oppression will transformed oasis justice\", \" \", \" dream four little children will one day live nation will judged color skin content character\", \\n\" \", \" dream today\", \" \", \" dream one day alabama vicious racists governor lips dripping words interposition nullification one day right alabama little black boys black girls will able join hands little white boys white girls sisters brothers\", \" \", \" dream today\", \" \", \" dream one day every valley shall exalted every hill mountain shall made low rough places will made plain crooked places will made straight glory lord shall revealed flesh shall see \", \" \", \" hope faith go back south \", \" \", \" faith will able hew mountain despair stone hope faith will able transform jangling discords nation beautiful symphony brotherhood faith will able work pray struggle go jail stand knowing will free one day\", \\n\" \", \" will day will day god s children will able sing new meaning\", \" \", \" country tis thee sweet land liberty thee sing\", \"land fathers died land pilgrim s pride\", \" every mountainside let ring\", \" america great nation must become true\", \" let ring prodigious hilltops new hampshire\", \"let ring mighty mountains new york\", \"let ring heightening alleghenies pennsylvania\", \"let ring snowcapped rockies colorado\", \"let ring curvaceous slopes california\", \" \", \" \", \"let ring stone mountain georgia\", \"let ring lookout mountain tennessee\", \\n\"let ring every hill molehill mississippi\", \" every mountainside let ring\", \" happens allow ring let ring every village every hamlet every state every city will able speed day god s children black men white men jews gentiles protestants catholics will able join hands sing words old negro spiritual\", \"free last free last\", \" \", \"thank god almighty free last\", \"\")",
        "\n[2] list(language = \"en\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ",
        "\n[3] list()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ",
        "\n",
        "> ",
        "",
        "> ",
        "#Replacing \"/\", \"@\" and \"|\" with space:",
        "> ",
        "toSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))",
        "> ",
        "docs <- tm_map(docs, toSpace, \"/\")",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, toSpace, \"/\") :",
        " transformation drops documents\n",
        "> ",
        "docs <- tm_map(docs, toSpace, \"@\")",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, toSpace, \"@\") :",
        " transformation drops documents\n",
        "> ",
        "docs <- tm_map(docs, toSpace, \"\\\\|\")",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, toSpace, \"\\\\|\") :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "#Cleaning the text",
        "> ",
        "# Convert the text to lower case",
        "> ",
        "docs <- tm_map(docs, content_transformer(tolower))",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, content_transformer(tolower)) :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "#Remove punctuation",
        "> ",
        "docs <- tm_map(docs, toSpace, \"[[:punct:]]\")",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, toSpace, \"[[:punct:]]\") :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "#Remove numbers",
        "> ",
        "docs <- tm_map(docs, toSpace, \"[[:digit:]]\")",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, toSpace, \"[[:digit:]]\") :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "# add two extra stop words: \"available\" and \"via\"",
        "> ",
        "myStopwords = readLines(\"stopword_en.csv\")",
        "Error in file(con, \"r\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(con, \"r\") :",
        "\n ",
        " cannot open file 'stopword_en.csv': No such file or directory\n",
        "> ",
        "",
        "> ",
        "# remove stopwords from corpus",
        "> ",
        "docs <- tm_map(docs, removeWords, myStopwords)",
        "Error in sort(words, decreasing = TRUE) : object 'myStopwords' not found\n",
        "> ",
        "",
        "> ",
        "# Remove your own stop word",
        "> ",
        "# specify your stopwords as a character vector",
        "> ",
        "docs <- tm_map(docs, removeWords, c(\"flight\",\"you\",\"air\",\"lion\",\"airline\",\"reviewed\")) ",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, removeWords, c(\"flight\", \"you\", \"air\",  :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "# Eliminate extra white spaces",
        "> ",
        "docs <- tm_map(docs, stripWhitespace)",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, stripWhitespace) :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "# Remove URL",
        "> ",
        "removeURL <- function(x) gsub(\"http[[:alnum:]]*\", \" \", x)",
        "> ",
        "docs <- tm_map(docs, removeURL)",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, removeURL) :",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "#Replace words",
        "> ",
        "docs <- tm_map(docs, gsub, pattern=\"Howver\", replacement=\"However\")",
        "Warning message:\n",
        "In tm_map.SimpleCorpus(docs, gsub, pattern = \"Howver\", replacement = \"However\") :",
        "\n ",
        " transformation drops documents\n",
        "> ",
        "",
        "> ",
        "#Build a term-document matrix",
        "> ",
        "dtm <- TermDocumentMatrix(docs)",
        "> ",
        "m <- as.matrix(dtm)",
        "> ",
        "v <- sort(rowSums(m),decreasing=TRUE)",
        "> ",
        "d <- data.frame(word = names(v),freq=v)",
        "> ",
        "head(d, 15)",
        "             word freq\nwill         will   17\nring         ring   12\nday           day   11\ndream       dream   11\nlet           let   11\nevery       every    9\nable         able    8\none           one    8\nfaith       faith    4\nfree         free    4\nmountain mountain    4\nnation     nation    4\nshall       shall    4\nblack       black    3\nchildren children    3\n",
        "> ",
        "",
        "> ",
        "#Generate the Word cloud",
        "> ",
        "set.seed(1234)",
        "> ",
        "wordcloud(words = d$word, freq = d$freq, min.freq = 1,",
        "+ ",
        "          max.words=50, random.order=FALSE, rot.per=0.35, ",
        "+ ",
        "          colors=brewer.pal(8, \"Dark2\"))",
        "> ",
        "",
        "> ",
        "",
        "> ",
        "dataframe<-data.frame(text=unlist(sapply(docs, `[`)), stringsAsFactors=F)",
        "> ",
        "",
        "> ",
        "write.csv(dataframe, \"F:/DOC/lionGithub/lion2.csv\")",
        "Error in file(file, ifelse(append, \"a\", \"w\")) : \n  cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(file, ifelse(append, \"a\", \"w\")) :",
        "\n ",
        " cannot open file 'F:/DOC/lionGithub/lion2.csv': No such file or directory\n",
        "> ",
        "save.image()",
        "> ",
        "",
        "> ",
        "",
        "> ",
        "",
        "> ",
        "",
        "> ",
        "#tahap 2",
        "> ",
        "",
        "> ",
        "setwd(\"F:/DOC/lionGithub\")",
        "Error in setwd(\"F:/DOC/lionGithub\") : cannot change working directory\n",
        "> ",
        "kalimat2<-read.csv(\"lion2.csv\",header=TRUE)",
        "Error in file(file, \"rt\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(file, \"rt\") :",
        "\n ",
        " cannot open file 'lion2.csv': No such file or directory\n",
        "> ",
        "",
        "> ",
        "#skoring",
        "> ",
        "positif <- scan(\"F:/DOC/lionGithub/positive-words.txt\",what=\"character\",comment.char=\";\")",
        "Error in file(file, \"r\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(file, \"r\") :",
        "\n ",
        " cannot open file 'F:/DOC/lionGithub/positive-words.txt': No such file or directory\n",
        "> ",
        "negatif <- scan(\"F:/DOC/lionGithub/negative-words.txt\",what=\"character\",comment.char=\";\")",
        "Error in file(file, \"r\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(file, \"r\") :",
        "\n ",
        " cannot open file 'F:/DOC/lionGithub/negative-words.txt': No such file or directory\n",
        "> ",
        "kata.positif = c(positif, \"is near to\")",
        "Error: object 'positif' not found\n",
        "> ",
        "kata.negatif = c(negatif, \"cant\")",
        "Error: object 'negatif' not found\n",
        "> ",
        "score.sentiment = function(kalimat2, kata.positif, kata.negatif, .progress='none')",
        "+ ",
        "{",
        "+ ",
        "  require(plyr)",
        "+ ",
        "  require(stringr)",
        "+ ",
        "  scores = laply(kalimat2, function(kalimat, kata.positif, kata.negatif) {",
        "+ ",
        "    kalimat = gsub('[[:punct:]]', '', kalimat)",
        "+ ",
        "    kalimat = gsub('[[:cntrl:]]', '', kalimat)",
        "+ ",
        "    kalimat = gsub('\\\\d+', '', kalimat)",
        "+ ",
        "    kalimat = tolower(kalimat)",
        "+ ",
        "    ",
        "+ ",
        "    list.kata = str_split(kalimat, '\\\\s+')",
        "+ ",
        "    kata2 = unlist(list.kata)",
        "+ ",
        "    positif.matches = match(kata2, kata.positif)",
        "+ ",
        "    negatif.matches = match(kata2, kata.negatif)",
        "+ ",
        "    positif.matches = !is.na(positif.matches)",
        "+ ",
        "    negatif.matches = !is.na(negatif.matches)",
        "+ ",
        "    score = sum(positif.matches) - (sum(negatif.matches))",
        "+ ",
        "    return(score)",
        "+ ",
        "  }, kata.positif, kata.negatif, .progress=.progress )",
        "+ ",
        "  scores.df = data.frame(score=scores, text=kalimat2)",
        "+ ",
        "  return(scores.df)",
        "+ ",
        "}",
        "> ",
        "",
        "> ",
        "hasil = score.sentiment(kalimat2$text, kata.positif, kata.negatif)",
        "Loading required package: plyr\n",
        "Error in laply(kalimat2, function(kalimat, kata.positif, kata.negatif) { : \n  object 'kalimat2' not found\n",
        "> ",
        "View(hasil)",
        "Error in View : object 'hasil' not found\n",
        "> ",
        "",
        "> ",
        "#CONVERT SCORE TO SENTIMENT",
        "> ",
        "hasil$klasifikasi<- ifelse(hasil$score<0, \"Negatif\",ifelse(hasil$score==0,\"Netral\",\"Positif\"))",
        "Error in ifelse(hasil$score < 0, \"Negatif\", ifelse(hasil$score == 0, \"Netral\",  : \n  object 'hasil' not found\n",
        "> ",
        "hasil$klasifikasi",
        "Error: object 'hasil' not found\n",
        "> ",
        "View(hasil)",
        "\nRestarting R session...\n\n"
    ]
}